{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Training LSTM model provided for the exam."
      ],
      "metadata": {
        "id": "iBkebESyZ_mx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sE8g8xwdOhbD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0278d14c-9ad4-484d-8d81-3388575c5dcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-1d362e5c76e9>:21: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  row[0] = row[0].replace('rt', ' ')\n",
            "<ipython-input-6-1d362e5c76e9>:21: FutureWarning: Series.__setitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To set a value by position, use `ser.iloc[pos] = value`\n",
            "  row[0] = row[0].replace('rt', ' ')\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "291/291 - 31s - 107ms/step - accuracy: 0.6408 - loss: 0.8318\n",
            "144/144 - 3s - 22ms/step - accuracy: 0.6774 - loss: 0.7735\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7734556198120117\n",
            "0.677370011806488\n",
            "['loss', 'compile_metrics']\n",
            "Tokenizer saved as tokenizer.pickle\n",
            "Label encoder classes saved as label_encoder_classes.npy: ['Negative' 'Neutral' 'Positive']\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from matplotlib import pyplot\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import re\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "data = pd.read_csv('Data.csv')\n",
        "# Keeping only the neccessary columns\n",
        "data = data[['text','sentiment']]\n",
        "\n",
        "data['text'] = data['text'].apply(lambda x: x.lower())\n",
        "data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]', '', x)))\n",
        "\n",
        "for idx, row in data.iterrows():\n",
        "    row[0] = row[0].replace('rt', ' ')\n",
        "\n",
        "max_fatures = 2000\n",
        "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
        "tokenizer.fit_on_texts(data['text'].values)\n",
        "X = tokenizer.texts_to_sequences(data['text'].values)\n",
        "\n",
        "X = pad_sequences(X)\n",
        "\n",
        "embed_dim = 128\n",
        "lstm_out = 196\n",
        "def createmodel():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\n",
        "    model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(Dense(3,activation='softmax'))\n",
        "    model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "    return model\n",
        "# print(model.summary())\n",
        "\n",
        "labelencoder = LabelEncoder()\n",
        "integer_encoded = labelencoder.fit_transform(data['sentiment'])\n",
        "y = to_categorical(integer_encoded)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,y, test_size = 0.33, random_state = 42)\n",
        "\n",
        "batch_size = 32\n",
        "model = createmodel()\n",
        "model.fit(X_train, Y_train, epochs = 1, batch_size=batch_size, verbose = 2)\n",
        "score,acc = model.evaluate(X_test,Y_test,verbose=2,batch_size=batch_size)\n",
        "print(score)\n",
        "print(acc)\n",
        "print(model.metrics_names)\n",
        "\n",
        "model.save('./sentiment_model' + '.h5')\n",
        "# Also, you'll need to save your tokenizer, as it's crucial for preprocessing new text.\n",
        "# The easiest way is using pickle.\n",
        "import pickle\n",
        "\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "print(\"Tokenizer saved as tokenizer.pickle\")\n",
        "\n",
        "# (After labelencoder has been fitted)\n",
        "import numpy as np\n",
        "np.save('label_encoder_classes.npy', labelencoder.classes_)\n",
        "print(f\"Label encoder classes saved as label_encoder_classes.npy: {labelencoder.classes_}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1**: Execute and save the given model and use the saved model to predict\n",
        "on new text data (ex, “A lot of good things are happening. We are respected again throughout the world, and that's a great thing .@realDonaldTrump”)"
      ],
      "metadata": {
        "id": "Fuq2Ua0Tafke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pickle\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# --- Configuration & File Paths ---\n",
        "MODEL_PATH = 'sentiment_model.h5'\n",
        "TOKENIZER_PATH = 'tokenizer.pickle'\n",
        "LABEL_CLASSES_PATH = 'label_encoder_classes.npy'\n",
        "\n",
        "# --- Load Saved Artifacts ---\n",
        "try:\n",
        "    # Load the trained model\n",
        "    loaded_model = load_model(MODEL_PATH)\n",
        "    print(f\"Model loaded successfully from {MODEL_PATH}\")\n",
        "\n",
        "    # Load the tokenizer\n",
        "    with open(TOKENIZER_PATH, 'rb') as handle:\n",
        "        loaded_tokenizer = pickle.load(handle)\n",
        "    print(f\"Tokenizer loaded successfully from {TOKENIZER_PATH}\")\n",
        "\n",
        "    # Load the label encoder classes\n",
        "    label_classes = np.load(LABEL_CLASSES_PATH, allow_pickle=True)\n",
        "    print(f\"Label encoder classes loaded successfully from {LABEL_CLASSES_PATH}: {label_classes}\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error: Could not find a required file. Make sure '{MODEL_PATH}', '{TOKENIZER_PATH}', and '{LABEL_CLASSES_PATH}' exist in the current directory.\")\n",
        "    print(f\"Details: {e}\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during loading: {e}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# --- Determine Input Sequence Length ---\n",
        "# This should match the input_length used during training (X.shape[1] in your training code)\n",
        "# We can often infer this from the model's first layer (Embedding layer)\n",
        "try:\n",
        "    input_seq_length = loaded_model.input_shape[1]\n",
        "    if input_seq_length is None: # Should not happen for this model structure but good to check\n",
        "        raise ValueError(\"Could not determine input_seq_length from model's input shape.\")\n",
        "    print(f\"Inferred input sequence length from model: {input_seq_length}\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not automatically determine input_seq_length from model: {e}\")\n",
        "    # Fallback: If you know the exact length from your training script's X.shape[1]\n",
        "    # (the value of X.shape[1] AFTER `X = pad_sequences(X)` in training)\n",
        "    # you can hardcode it here. For example:\n",
        "    # input_seq_length = 200 # Replace 200 with your actual value if needed\n",
        "    print(\"Please ensure 'input_seq_length' is set correctly if auto-detection failed.\")\n",
        "    # For your specific training code, `X = pad_sequences(X)` means the length is determined\n",
        "    # by the longest sequence. The Embedding layer was `Embedding(..., input_length=X.shape[1])`.\n",
        "    # So, `loaded_model.input_shape[1]` is the correct way.\n",
        "\n",
        "# --- Preprocessing Function for New Text ---\n",
        "def preprocess_text_for_prediction(text_input, tokenizer, max_len):\n",
        "    \"\"\"\n",
        "    Preprocesses a single raw text string for prediction.\n",
        "    Mirrors the preprocessing steps from the training script.\n",
        "    \"\"\"\n",
        "    # 1. Lowercase\n",
        "    processed_text = text_input.lower()\n",
        "    # 2. Remove special characters (same regex as training)\n",
        "    processed_text = re.sub(r'[^a-zA-z0-9\\s]', '', processed_text)\n",
        "    # 3. Replace 'rt' (same as training)\n",
        "    processed_text = processed_text.replace('rt', ' ') # remove \"rt \"\n",
        "    processed_text = processed_text.strip() # Clean up extra spaces\n",
        "\n",
        "    # Tokenize the text\n",
        "    sequence = tokenizer.texts_to_sequences([processed_text]) # Note: texts_to_sequences expects a list\n",
        "\n",
        "    # Pad the sequence\n",
        "    padded_sequence = pad_sequences(sequence, maxlen=max_len)\n",
        "\n",
        "    return padded_sequence\n",
        "\n",
        "# --- Prediction Function ---\n",
        "def predict_sentiment(raw_text):\n",
        "    \"\"\"\n",
        "    Predicts sentiment for a given raw text string.\n",
        "    \"\"\"\n",
        "    if not all([loaded_model, loaded_tokenizer, label_classes is not None, input_seq_length is not None]):\n",
        "        print(\"Error: Model, tokenizer, label classes, or input_seq_length not initialized.\")\n",
        "        return None, None\n",
        "\n",
        "    # Preprocess the input text\n",
        "    preprocessed_input = preprocess_text_for_prediction(raw_text, loaded_tokenizer, input_seq_length)\n",
        "\n",
        "    # Make prediction\n",
        "    prediction_probabilities = loaded_model.predict(preprocessed_input)\n",
        "\n",
        "    # Get the class with the highest probability\n",
        "    predicted_class_index = np.argmax(prediction_probabilities, axis=1)[0] # Get the single index\n",
        "\n",
        "    # Map index to sentiment label\n",
        "    predicted_sentiment_label = label_classes[predicted_class_index]\n",
        "\n",
        "    return predicted_sentiment_label, prediction_probabilities[0]\n",
        "\n",
        "\n",
        "# --- Example Usage ---\n",
        "if __name__ == \"__main__\":\n",
        "    new_text_example = \"A lot of good things are happening. We are respected again throughout the world, and that's a great thing .@realDonaldTrump\"\n",
        "    texts_to_test = [new_text_example]\n",
        "\n",
        "    for text in texts_to_test:\n",
        "        print(f\"\\nOriginal Text: \\\"{text}\\\"\")\n",
        "        sentiment, probabilities = predict_sentiment(text)\n",
        "        if sentiment and probabilities is not None:\n",
        "            print(f\"Predicted Sentiment: {sentiment}\")\n",
        "            print(f\"Prediction Probabilities: \")\n",
        "            for i, prob in enumerate(probabilities):\n",
        "                print(f\"  - {label_classes[i]}: {prob:.4f}\")\n",
        "        else:\n",
        "            print(\"Prediction failed.\")\n",
        "\n",
        "    # Example of predicting a single string directly\n",
        "    # text_to_predict_single = \"I love this product, it's amazing!\"\n",
        "    # sentiment, _ = predict_sentiment(text_to_predict_single)\n",
        "    # print(f\"\\nPrediction for '{text_to_predict_single}': {sentiment}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEPMpMj2UC1C",
        "outputId": "e6ddf6bf-c310-42d6-f674-064708cc1f7a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully from sentiment_model.h5\n",
            "Tokenizer loaded successfully from tokenizer.pickle\n",
            "Label encoder classes loaded successfully from label_encoder_classes.npy: ['Negative' 'Neutral' 'Positive']\n",
            "Inferred input sequence length from model: 28\n",
            "\n",
            "Original Text: \"A lot of good things are happening. We are respected again throughout the world, and that's a great thing .@realDonaldTrump\"\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "Predicted Sentiment: Negative\n",
            "Prediction Probabilities: \n",
            "  - Negative: 0.6006\n",
            "  - Neutral: 0.1928\n",
            "  - Positive: 0.2067\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikeras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1nkaerxVv2e",
        "outputId": "f407083f-77c1-4452-a824-2426cc898d80"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikeras\n",
            "  Downloading scikeras-0.13.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from scikeras) (3.8.0)\n",
            "Requirement already satisfied: scikit-learn>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from scikeras) (1.6.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.0.9)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (3.13.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.15.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (24.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.2->scikeras) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras>=3.2.0->scikeras) (4.13.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.2.0->scikeras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.2.0->scikeras) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n",
            "Downloading scikeras-0.13.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: scikeras\n",
            "Successfully installed scikeras-0.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# TensorFlow / Keras imports (ensure these are from tensorflow.keras)\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout # Added Dropout for flexibility\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam # To control learning rate\n",
        "\n",
        "# KerasClassifier wrapper\n",
        "# NOTE: tensorflow.keras.wrappers.scikit_learn is for older TensorFlow versions.\n",
        "# For TensorFlow 2.7+, it's recommended to use scikeras:\n",
        "# 1. Install: pip install scikeras\n",
        "# 2. Import: from scikeras.wrappers import KerasClassifier\n",
        "# However, I will use the import you provided in your example:\n",
        "from scikeras.wrappers import KerasClassifier"
      ],
      "metadata": {
        "id": "Fa0gVM2QVdXp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U scikit-learn scikeras tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZYbxnNNiYDVU",
        "outputId": "26f44149-f742-48b2-a0d2-2c1c8d584c5b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: scikeras in /usr/local/lib/python3.11/dist-packages (0.13.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from scikeras) (3.8.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
            "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
            "  Downloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.2.0->scikeras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.2.0->scikeras) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n",
            "Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m115.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m118.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ml-dtypes, tensorboard, tensorflow\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.1\n",
            "    Uninstalling ml-dtypes-0.4.1:\n",
            "      Successfully uninstalled ml-dtypes-0.4.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.18.0\n",
            "    Uninstalling tensorboard-2.18.0:\n",
            "      Successfully uninstalled tensorboard-2.18.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.18.0\n",
            "    Uninstalling tensorflow-2.18.0:\n",
            "      Successfully uninstalled tensorflow-2.18.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.19.0 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.19.0 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.19.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed ml-dtypes-0.5.1 tensorboard-2.19.0 tensorflow-2.19.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "ml_dtypes",
                  "tensorflow"
                ]
              },
              "id": "51f0419e8d0a47d79bd85ea83b364ba0"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y scikit-learn\n",
        "!pip install scikit-learn==1.5.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TO04C0gaMyt",
        "outputId": "d42c4fe3-4672-470b-d5ad-5aa69c5edf15"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: scikit-learn 1.6.1\n",
            "Uninstalling scikit-learn-1.6.1:\n",
            "  Successfully uninstalled scikit-learn-1.6.1\n",
            "Collecting scikit-learn==1.5.2\n",
            "  Downloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (3.6.0)\n",
            "Downloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m112.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-learn\n",
            "Successfully installed scikit-learn-1.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "# TensorFlow / Keras imports\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D # Added SpatialDropout1D if you want to tune it\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Input\n",
        "\n",
        "# SciKeras wrapper (ensure you have it installed: pip install scikeras)\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "\n",
        "# --- 1. Load and Prepare Your Data (Same as your original script) ---\n",
        "try:\n",
        "    data = pd.read_csv('Data.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'Data.csv' not found. Please make sure the file exists.\")\n",
        "    exit()\n",
        "\n",
        "data = data[['text','sentiment']]\n",
        "data['text'] = data['text'].apply(lambda x: x.lower())\n",
        "data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]', '', x)))\n",
        "for idx, row in data.iterrows():\n",
        "    row[0] = row[0].replace('rt', ' ')\n",
        "\n",
        "max_features = 2000 # Vocabulary size from your original code\n",
        "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
        "tokenizer.fit_on_texts(data['text'].values)\n",
        "X_seq = tokenizer.texts_to_sequences(data['text'].values)\n",
        "X_pad = pad_sequences(X_seq) # Padding to the length of the longest sequence\n",
        "\n",
        "# Store the input_length for the Embedding layer\n",
        "input_seq_length = X_pad.shape[1] # This is X.shape[1] from your original code after padding\n",
        "print(f\"Input sequence length (max_len for padding): {input_seq_length}\")\n",
        "\n",
        "# Prepare labels\n",
        "labelencoder = LabelEncoder()\n",
        "integer_encoded = labelencoder.fit_transform(data['sentiment'])\n",
        "y_cat = to_categorical(integer_encoded)\n",
        "num_classes = y_cat.shape[1] # Number of unique sentiment classes (should be 3 based on your original model)\n",
        "print(f\"Number of classes: {num_classes}\")\n",
        "print(f\"Label encoder classes: {labelencoder.classes_}\")\n",
        "\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_pad, y_cat, test_size=0.33, random_state=42)\n",
        "print(f\"X_train shape: {X_train.shape}, Y_train shape: {Y_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, Y_test shape: {Y_test.shape}\")\n",
        "\n",
        "\n",
        "# --- 2. Create a `build_model` Function for GridSearchCV (LSTM Model) ---\n",
        "def build_lstm_model_for_grid(\n",
        "    embedding_dim=128,\n",
        "    lstm_units=196,\n",
        "    lstm_dropout=0.2,\n",
        "    lstm_recurrent_dropout=0.2,\n",
        "    learning_rate=0.001\n",
        "):\n",
        "    model = Sequential([\n",
        "        Input(shape=(input_seq_length,)),             # ← ensures model.outputs exists\n",
        "        Embedding(input_dim=max_features,\n",
        "                  output_dim=embedding_dim,\n",
        "                  input_length=input_seq_length),\n",
        "        LSTM(lstm_units,\n",
        "             dropout=lstm_dropout,\n",
        "             recurrent_dropout=lstm_recurrent_dropout),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=learning_rate),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# ————————————————\n",
        "# 3. Wrap for GridSearch\n",
        "# ————————————————\n",
        "model_for_grid = KerasClassifier(\n",
        "    model=build_lstm_model_for_grid,\n",
        "    verbose=0,\n",
        ")\n",
        "\n",
        "param_grid = {\n",
        "    'batch_size': [32, 64],\n",
        "    'epochs':     [5, 7],\n",
        "    'model__embedding_dim': [64, 128],\n",
        "}\n",
        "print(\"\\nParameter grid for GridSearchCV (LSTM Model):\")\n",
        "for key, value in param_grid.items():\n",
        "    print(f\"- {key}: {value}\")\n",
        "\n",
        "# --- 4. Perform GridSearchCV ---\n",
        "grid = GridSearchCV(\n",
        "    estimator=model_for_grid,\n",
        "    param_grid=param_grid,\n",
        "    cv=2,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "print(\"\\nStarting GridSearchCV with LSTM model... This may take a while.\")\n",
        "try:\n",
        "    grid_result = grid.fit(X_train, Y_train)\n",
        "\n",
        "    # --- 5. Display Results and Evaluate the Best Model ---\n",
        "    print(f\"\\nBest Score (Cross-Validation Accuracy): {grid_result.best_score_:.4f}\")\n",
        "    print(f\"Best Hyperparameters: {grid_result.best_params_}\")\n",
        "\n",
        "    print(\"\\nAll results from GridSearchCV:\")\n",
        "    means = grid_result.cv_results_['mean_test_score']\n",
        "    stds = grid_result.cv_results_['std_test_score']\n",
        "    params_list = grid_result.cv_results_['params']\n",
        "    for mean, stdev, param_combination in zip(means, stds, params_list):\n",
        "        print(f\"Mean CV Accuracy: {mean:.4f} (Std: {stdev:.4f}) with: {param_combination}\")\n",
        "\n",
        "    best_model_wrapper = grid_result.best_estimator_\n",
        "\n",
        "    if hasattr(best_model_wrapper, 'model_'):\n",
        "        best_keras_model_actual = best_model_wrapper.model_\n",
        "        loss, accuracy = best_keras_model_actual.evaluate(X_test, Y_test, verbose=0)\n",
        "        print(f\"\\nTest Accuracy of the Best LSTM Model on X_test, Y_test: {accuracy:.4f}\")\n",
        "        print(f\"Test Loss of the Best LSTM Model on X_test, Y_test: {loss:.4f}\")\n",
        "    else:\n",
        "        print(\"\\nEvaluating best LSTM model on test set using the wrapper's score method:\")\n",
        "        test_accuracy = best_model_wrapper.score(X_test, Y_test)\n",
        "        print(f\"Test Accuracy from best_estimator_.score(): {test_accuracy:.4f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during GridSearchCV fitting or evaluation: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzGEzmllZSka",
        "outputId": "f84a234e-3cb7-4bd6-8f3c-604a982f26e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-3b2eb589d182>:30: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  row[0] = row[0].replace('rt', ' ')\n",
            "<ipython-input-3-3b2eb589d182>:30: FutureWarning: Series.__setitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To set a value by position, use `ser.iloc[pos] = value`\n",
            "  row[0] = row[0].replace('rt', ' ')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input sequence length (max_len for padding): 28\n",
            "Number of classes: 3\n",
            "Label encoder classes: ['Negative' 'Neutral' 'Positive']\n",
            "X_train shape: (9293, 28), Y_train shape: (9293, 3)\n",
            "X_test shape: (4578, 28), Y_test shape: (4578, 3)\n",
            "\n",
            "Parameter grid for GridSearchCV (LSTM Model):\n",
            "- batch_size: [32, 64]\n",
            "- epochs: [5, 7]\n",
            "- model__embedding_dim: [64, 128]\n",
            "\n",
            "Starting GridSearchCV with LSTM model... This may take a while.\n",
            "Fitting 2 folds for each of 8 candidates, totalling 16 fits\n"
          ]
        }
      ]
    }
  ]
}